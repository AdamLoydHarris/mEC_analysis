{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "img = cv2.imread(\"/Users/AdamHarris/Desktop/detected_octagons.jpg\")\n",
    "if img is None:\n",
    "    raise FileNotFoundError(\"Image not found. Make sure 'maze.jpg' is in the working directory.\")\n",
    "\n",
    "# Convert to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply a binary threshold\n",
    "# You might need to experiment with the threshold value (e.g., 100, 120, 150)\n",
    "# or try cv2.THRESH_OTSU for an adaptive approach.\n",
    "_, thresh = cv2.threshold(gray, 120, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Optional: Morphological operations to reduce noise and solidify shapes\n",
    "# For example, we can use a closing operation (dilate then erode) \n",
    "# to fill small holes within the platforms.\n",
    "kernel = np.ones((3,3), np.uint8)\n",
    "closed = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "\n",
    "# Find contours of the platforms\n",
    "# RETR_EXTERNAL will only find external contours which is likely sufficient \n",
    "# if the platforms are well-separated.\n",
    "# CHAIN_APPROX_SIMPLE compresses horizontal, vertical, and diagonal segments \n",
    "# and leaves only their end points.\n",
    "contours, hierarchy = cv2.findContours(closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Filter out small contours that are not likely to be platforms\n",
    "# You may adjust this area threshold depending on your image scale\n",
    "min_area = 5000\n",
    "platform_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_area]\n",
    "\n",
    "# Draw the contours on a copy of the original image for visualization\n",
    "img_contours = img.copy()\n",
    "cv2.drawContours(img_contours, platform_contours, -1, (0,0,255), 2)\n",
    "\n",
    "# Create a binary mask for each platform\n",
    "masks = []\n",
    "for i, cnt in enumerate(platform_contours):\n",
    "    # Create a blank mask\n",
    "    mask = np.zeros_like(gray)\n",
    "    # Fill the contour on the mask\n",
    "    cv2.drawContours(mask, [cnt], -1, 255, -1)\n",
    "    masks.append(mask)\n",
    "\n",
    "    # Optionally, save each mask as an image\n",
    "    # Each mask highlights one platform\n",
    "    cv2.imwrite(f\"/Users/AdamHarris/Desktop/platform_mask_{i}.png\", mask)\n",
    "\n",
    "# If you want a single mask highlighting all platforms:\n",
    "all_platforms_mask = np.zeros_like(gray)\n",
    "for cnt in platform_contours:\n",
    "    cv2.drawContours(all_platforms_mask, [cnt], -1, 255, -1)\n",
    "cv2.imwrite(\"/Users/AdamHarris/Desktop/all_platforms_mask.png\", all_platforms_mask)\n",
    "\n",
    "# Display results (press any key to close windows)\n",
    "cv2.imshow(\"Original Image\", img)\n",
    "cv2.imshow(\"Thresholded\", closed)\n",
    "cv2.imshow(\"Contours\", img_contours)\n",
    "cv2.imshow(\"All Platforms Mask\", all_platforms_mask)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "os.chdir('/Users/AdamHarris/Desktop/')\n",
    "# Load the already segmented binary mask (where platforms are white)\n",
    "# Assume this is a single-channel binary image (0 or 255)\n",
    "binary = cv2.imread(\"all_platforms_mask.png\", cv2.IMREAD_GRAYSCALE)\n",
    "if binary is None:\n",
    "    raise FileNotFoundError(\"Could not find segmented_mask.png\")\n",
    "\n",
    "# A function to compute circularity given a contour\n",
    "def compute_circularity(cnt):\n",
    "    area = cv2.contourArea(cnt)\n",
    "    perimeter = cv2.arcLength(cnt, True)\n",
    "    if perimeter == 0:\n",
    "        return 0\n",
    "    circularity = (4 * np.pi * area) / (perimeter * perimeter)\n",
    "    return circularity\n",
    "\n",
    "# Find initial contours\n",
    "contours, hierarchy = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Parameters you may need to adjust:\n",
    "min_area = 5000       # minimum area a valid platform should have\n",
    "circularity_threshold = 0.6  # if contour's circularity < 0.6, consider it might be multiple platforms joined\n",
    "\n",
    "# Expected grid: 3x3\n",
    "# Let's say we know approximate coordinates for each platform (center positions in the image)\n",
    "# You should measure these from your image (example placeholders here):\n",
    "image_height, image_width = binary.shape\n",
    "# Hypothetical approximate grid coordinates (cx, cy) for each platform:\n",
    "# You need to determine these coordinates based on your setup.\n",
    "# For demonstration, let's assume they're spaced evenly. \n",
    "grid_x = np.linspace(image_width*0.2, image_width*0.8, 3)   # adjust as needed\n",
    "grid_y = np.linspace(image_height*0.2, image_height*0.8, 3) # adjust as needed\n",
    "expected_centers = [(int(x), int(y)) for y in grid_y for x in grid_x]\n",
    "\n",
    "# Letâ€™s store final masks of individual platforms\n",
    "final_masks = []\n",
    "\n",
    "for cnt in contours:\n",
    "    area = cv2.contourArea(cnt)\n",
    "    if area < min_area:\n",
    "        # Too small to be a platform; ignore\n",
    "        continue\n",
    "\n",
    "    # Check circularity\n",
    "    circ = compute_circularity(cnt)\n",
    "    \n",
    "    # Extract the ROI of this contour to apply watershed if needed\n",
    "    x, y, w, h = cv2.boundingRect(cnt)\n",
    "    contour_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    cv2.drawContours(contour_mask, [cnt - [x,y]], -1, 255, -1)  # draw contour localized in ROI\n",
    "\n",
    "    if circ < circularity_threshold:\n",
    "        # This suggests multiple platforms stuck together.\n",
    "        # Use distance transform and watershed to separate.\n",
    "        \n",
    "        # Distance transform\n",
    "        dist = cv2.distanceTransform(contour_mask, cv2.DIST_L2, 5)\n",
    "        # Normalize for visualization/thresholding\n",
    "        dist_norm = cv2.normalize(dist, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "        \n",
    "        # Threshold to find peaks\n",
    "        ret, peaks = cv2.threshold(dist_norm, 200, 255, cv2.THRESH_BINARY)\n",
    "        # Find markers\n",
    "        markers_contours, _ = cv2.findContours(peaks, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        markers = np.zeros_like(dist, dtype=np.int32)\n",
    "        for i, mc in enumerate(markers_contours, 1):\n",
    "            cv2.drawContours(markers, [mc], -1, i, -1)\n",
    "        \n",
    "        # Apply watershed on the ROI\n",
    "        # Convert ROI to BGR for watershed\n",
    "        roi_bgr = cv2.cvtColor(contour_mask, cv2.COLOR_GRAY2BGR)\n",
    "        cv2.watershed(roi_bgr, markers)\n",
    "        \n",
    "        # Each marker > 1 corresponds to a separated object\n",
    "        for m_id in range(1, len(markers_contours)+1):\n",
    "            mask_temp = np.zeros_like(contour_mask)\n",
    "            mask_temp[markers == m_id] = 255\n",
    "            # Add back offset\n",
    "            full_mask = np.zeros_like(binary)\n",
    "            full_mask[y:y+h, x:x+w] = mask_temp\n",
    "            final_masks.append(full_mask)\n",
    "    else:\n",
    "        # Contour is already fairly circular; likely a single platform\n",
    "        # Just add this as a final mask\n",
    "        single_mask = np.zeros_like(binary)\n",
    "        cv2.drawContours(single_mask, [cnt], -1, 255, -1)\n",
    "        final_masks.append(single_mask)\n",
    "\n",
    "# At this point, we have a list of individual platform masks, but we might have missed or occluded platforms.\n",
    "\n",
    "# Check if we have 9 platforms:\n",
    "if len(final_masks) < 9:\n",
    "    # We know it should be a 3x3 arrangement.\n",
    "    # Identify which expected positions are unoccupied.\n",
    "    # One approach: For each expected center, check if there's a mask covering it.\n",
    "    final_combined = np.zeros_like(binary)\n",
    "    for m in final_masks:\n",
    "        final_combined = cv2.bitwise_or(final_combined, m)\n",
    "\n",
    "    for (ex_cx, ex_cy) in expected_centers:\n",
    "        if final_combined[ex_cy, ex_cx] == 0:\n",
    "            # No platform found at this location\n",
    "            # Attempt to \"force\" a platform shape here. For example, \n",
    "            # we can place a filled polygon or run a morphological operation \n",
    "            # to approximate a platform.\n",
    "            \n",
    "            # As a simple heuristic, we can draw a roughly octagonal shape at expected center:\n",
    "            # Adjust size and shape as needed\n",
    "            radius = 300  # platform \"radius\", tune this\n",
    "            missing_mask = np.zeros_like(binary)\n",
    "            cv2.circle(missing_mask, (ex_cx, ex_cy), radius, 255, -1)\n",
    "            \n",
    "            # Add this forced platform:\n",
    "            final_masks.append(missing_mask)\n",
    "\n",
    "# Combine all final masks into a single binary image\n",
    "all_final = np.zeros_like(binary)\n",
    "for m in final_masks:\n",
    "    all_final = cv2.bitwise_or(all_final, m)\n",
    "\n",
    "# (Optional) Refine shapes by slight morphological operations\n",
    "# For example, a closing to ensure nice round shapes:\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))\n",
    "all_final = cv2.morphologyEx(all_final, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "\n",
    "# Save the final result\n",
    "cv2.imwrite(\"corrected_platforms.png\", all_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def count_good_clusters(kilosort_folder):\n",
    "    \"\"\"\n",
    "    Count the number of clusters labeled as 'good' in Kilosort output.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    kilosort_folder : str\n",
    "        Path to the folder containing Kilosort output files.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The number of 'good' clusters.\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    FileNotFoundError\n",
    "        If neither cluster_info.tsv nor cluster_group.tsv can be found.\n",
    "    ValueError\n",
    "        If the expected 'group' or 'KSLabel' column is not present in the file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Potential Kilosort/Phy output files that contain cluster quality info\n",
    "    cluster_info_path = os.path.join(kilosort_folder, 'cluster_info.tsv')\n",
    "    cluster_group_path = os.path.join(kilosort_folder, 'cluster_group.tsv')\n",
    "    \n",
    "    # Attempt to load cluster_info.tsv first\n",
    "    if os.path.isfile(cluster_info_path):\n",
    "        df = pd.read_csv(cluster_info_path, sep='\\t')\n",
    "        # The 'group' column usually specifies cluster quality in newer versions of phy\n",
    "        # In older versions, 'KSLabel' is sometimes used to denote the same info.\n",
    "        if 'group' in df.columns:\n",
    "            good_clusters = df[df['group'] == 'good']\n",
    "        elif 'KSLabel' in df.columns:\n",
    "            good_clusters = df[df['KSLabel'] == 'good']\n",
    "        else:\n",
    "            raise ValueError(f\"No 'group' or 'KSLabel' column found in {cluster_info_path}\")\n",
    "        \n",
    "        return len(good_clusters)\n",
    "    \n",
    "    # If cluster_info.tsv not available, try cluster_group.tsv\n",
    "    elif os.path.isfile(cluster_group_path):\n",
    "        # cluster_group.tsv has a simpler format, usually two columns:\n",
    "        # cluster_id <tab> group_label\n",
    "        df = pd.read_csv(cluster_group_path, sep='\\t', header=None, names=['cluster_id', 'group'])\n",
    "        good_clusters = df[df['group'] == 'good']\n",
    "        return len(good_clusters)\n",
    "    \n",
    "    else:\n",
    "        # Neither cluster_info nor cluster_group found\n",
    "        raise FileNotFoundError(\"Neither cluster_info.tsv nor cluster_group.tsv found in the provided folder.\")\n",
    "\n",
    "# Example usage:\n",
    "# num_good = count_good_clusters('/path/to/kilosort/output')\n",
    "# print(f\"Number of good clusters: {num_good}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_good_clusters(kilosort_output_path: str) -> int:\n",
    "    \"\"\"\n",
    "    Count the number of 'good' clusters in a Kilosort output directory.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    kilosort_output_path : str\n",
    "        Path to the directory containing Kilosort output files.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The number of clusters labeled as 'good'.\n",
    "    \"\"\"\n",
    "    # Common files to look for. Adjust if you have a different naming scheme.\n",
    "    # This is the file that often contains cluster IDs and labels.\n",
    "    ks_label_file = os.path.join(kilosort_output_path, 'cluster_info.tsv')\n",
    "    \n",
    "    # If the Kilosort output uses a different file naming scheme, you may need\n",
    "    # to change this. For instance, if you only have 'cluster_info.tsv', you can\n",
    "    # search for a column that indicates whether a cluster is 'good' or not.\n",
    "    \n",
    "    # if not os.path.exists(ks_label_file):\n",
    "    #     raise FileNotFoundError(f\"No cluster_KSLabel.tsv file found at {ks_label_file}\")\n",
    "    \n",
    "    good_count = 0\n",
    "    with open(ks_label_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # Typical line format: \"cluster_id    label\"\n",
    "            parts = line.split()\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            # The second column is typically the cluster label\n",
    "            cluster_label = parts[2]\n",
    "            if cluster_label.lower() == 'good':\n",
    "                good_count += 1\n",
    "    \n",
    "    return good_count\n",
    "\n",
    "# Example usage:\n",
    "# path_to_kilosort = '/path/to/kilosort_output'\n",
    "# n_good = count_good_clusters(path_to_kilosort)\n",
    "# print(f\"Number of good clusters: {n_good}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/bp01_21032024_23032024_20241127122138_ALL_202411271709\n",
      "0\n",
      "/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/bp01_28032024_31032024_02042024_20241213161547_ALL_202412140004\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "folders = [\"/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/02042024_03042024_combined_all\", \n",
    "            \"/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/24032024_25032024_combined_all\",\n",
    "            \"/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/bp01_21032024_23032024_20241127122138_ALL_202411271709\",\n",
    "            \"/Volumes/behrens/adam_harris/Taskspace_abstraction_mEC/Data/cohort7/Sorted/bp01/bp01_28032024_31032024_02042024_20241213161547_ALL_202412140004\"]\n",
    "\n",
    "for i in folders[2:]:\n",
    "    print(i)\n",
    "    print(count_good_clusters(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
